{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOnNlsSLdo2wzjEKXZ8RFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjtm/deep-fake-voice-recognition/blob/feature%2Fstart/04_testing_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt0ixEOmSZlj",
        "outputId": "87530396-5925-495d-efd5-0601a8112d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'deep-fake-voice-recognition/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# !git clone -b feature/start https://github.com/fjtm/deep-fake-voice-recognition.git\n",
        "%cd deep-fake-voice-recognition/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz4fZY2weH6k",
        "outputId": "3b2a84b7-d829-4301-f324-8e7f7fb19078"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWZ-OiF4ZLBM",
        "outputId": "3ed5f526-0f72-4367-8e0e-a38a2148a192"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "gtKOMDnhTdYk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "0A4V14odTHyG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files_path = f'/content/drive/My Drive/deep-fake-voice-recognition/data/'\n",
        "train = pd.read_csv(csv_files_path+\"train.csv\")\n",
        "test = pd.read_csv(csv_files_path+\"test.csv\")"
      ],
      "metadata": {
        "id": "GKmiLHJjZTRA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = \"target\"\n",
        "index_columns = \"ind_num\"\n",
        "not_data_columns = [\"label\", \"ind\", \"target\",\"ind_num\"]"
      ],
      "metadata": {
        "id": "rK5xb4HCZcAY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(train.drop(not_data_columns, axis = 1).values).type(torch.float)\n",
        "y_train = torch.tensor(train[target_column].values).type(torch.float).view(-1, 1)\n",
        "X_test = torch.tensor(test.drop(not_data_columns, axis = 1).values).type(torch.float)\n",
        "y_test = torch.tensor(test[target_column].values).type(torch.float).view(-1, 1)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
        "# Calculate the minimum and maximum values for each row\n",
        "row_min = X_train.min(dim=0, keepdim=True).values\n",
        "row_max = X_train.max(dim=0, keepdim=True).values\n",
        "\n",
        "# Scale each row to the range [-1, 1]\n",
        "X_train = 2 * (X_train - row_min) / (row_max - row_min) - 1\n",
        "\n",
        "# Calculate the minimum and maximum values for each row\n",
        "row_min = X_test.min(dim=0, keepdim=True).values\n",
        "row_max = X_test.max(dim=0, keepdim=True).values\n",
        "\n",
        "# Scale each row to the range [-1, 1]\n",
        "X_test = 2 * (X_test - row_min) / (row_max - row_min) - 1"
      ],
      "metadata": {
        "id": "h2iiqgO2Ze0H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pQNBsXmDZhFD",
        "outputId": "684379e6-4240-44c5-9488-d64184069c54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_1 = nn.Linear(58, 112)  # Input size is 58, and the first hidden layer has 112 units\n",
        "        self.layer_2 = nn.Linear(112, 1)  # 2 units to the output layer with 2 units for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer_1(x))\n",
        "        x = self.layer_2(x)  # No activation function on the output for binary classification\n",
        "        return x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E37k0bhaQln",
        "outputId": "5e7a5c99-76df-466f-c100-d9f8ded3d4f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNN(\n",
              "  (layer_1): Linear(in_features=58, out_features=112, bias=True)\n",
              "  (layer_2): Linear(in_features=112, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loss function\n",
        "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
        "\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "f1_metric = BinaryF1Score().to(device)"
      ],
      "metadata": {
        "id": "Ras1VZA6cwbw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# Create a DataLoader\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10000, shuffle=False, num_workers=2)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=10000, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "-uUshv5hemip"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "w--ez9umhlC2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "model_0 = DNN().to(device)\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "       # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        model_0.train()\n",
        "        # 1. Forward pass\n",
        "        y_pred = model_0(X)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        if batch % 10 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss and accuracy\n",
        "    test_loss, test_f1 = 0, 0\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # 1. Forward pass\n",
        "            test_pred = model_0(X)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_f1 += f1_metric(target=y, preds=(test_pred >= 0.).float().view(-1, 1))\n",
        "\n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_f1 /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain f1: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test f1: {test_f1:.2f}%\\n\")\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(model_0.parameters()).device))"
      ],
      "metadata": {
        "id": "0IyYu09Bennr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}